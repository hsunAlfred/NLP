{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30618aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keras/.local/lib/python3.6/site-packages/pkg_resources/__init__.py:119: PkgResourcesDeprecationWarning: 0.18ubuntu0.18.04.1 is an invalid version and will not be supported in a future release\n",
      "  PkgResourcesDeprecationWarning,\n",
      "/home/keras/.local/lib/python3.6/site-packages/pkg_resources/_vendor/packaging/version.py:114: DeprecationWarning: Creating a LegacyVersion has been deprecated and will be removed in the next major release\n",
      "  DeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "from numpy import uint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "#import pandas as pd\n",
    "from joblib import dump\n",
    "from nlp_frame import nlp_frame\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import time\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd6937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c88de85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nlp_model_training(nlp_frame):\n",
    "    def __init__(self, vectParams: dict, segParams: dict, modelSelect: str, modelParams: dict) -> None:\n",
    "        '''\"vectParams\":dict\n",
    "        {\n",
    "            \"analyzer\":str \"word\" | \"char\" | \"char_wb\",\n",
    "            \"max_df\":float [0.0, 1.0],\n",
    "            \"min_df\":float [0.0, 1.0],\n",
    "            \"binary\":bool\n",
    "        }\n",
    "\n",
    "        \"segParams\":dict\n",
    "        {\n",
    "            \"corpus\":str,\n",
    "            \"HMM\":bool,\n",
    "            \"use_paddle\":bool\n",
    "        }\n",
    "\n",
    "        \"modelSelect\":str \"NB\" | \"RF\" | \"XG\",\n",
    "        \"modelParams\":dict\n",
    "        {\n",
    "            NB use, no Fool Proof, makesure what modelSelect you set\n",
    "            \"alpha\":float, default=1.0. Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "            \"fit_prior\":bool, default=True,Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "\n",
    "            RF use, no Fool Proof, makesure what modelSelect you set\n",
    "            \"n_estimators\":int, default=100\n",
    "            \"criterion\":str \"gini\" | \"entropy\" default=”gini”\n",
    "            \"min_samples_split\":int or float, default=2\n",
    "            \"min_samples_leaf\"int or float, default=1\n",
    "            \"max_features\":str \"auto\", \"sqrt\", \"log2\"\n",
    "            \"bootstrap\":bool, default=True\n",
    "            \"oob_scorebool\": bool, default=False, Only available if bootstrap=True.\n",
    "            \"class_weight\":default=None, {“balanced”, “balanced_subsample”} [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}]\n",
    "            \n",
    "            XG use, https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn\n",
    "            n_estimators: int,#總共迭代的次數，即決策樹的個數。預設值為100\n",
    "            max_depth: int,#樹的最大深度，默認值為6\n",
    "            booster: str,#Specify which booster to use: gbtree, gblinear or dart.\n",
    "            learning_rate: float,#學習速率，預設0.3\n",
    "            gamma: float,#懲罰項係數，指定節點分裂所需的最小損失函數下降值\n",
    "        }\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.vect = CountVectorizer(**vectParams)\n",
    "\n",
    "        self.segParams = segParams\n",
    "        self.modelSelect = modelSelect\n",
    "        self.modelParams = modelParams\n",
    "\n",
    "        # self.vect = HashingVectorizer(n_features=2**n)\n",
    "\n",
    "    def __loadCorpusAndSplit(self, corpus: str, HMM: bool, use_paddle: bool):\n",
    "        df = self.loadCorpus(corpus, HMM, use_paddle)\n",
    "\n",
    "        # BoW transform\n",
    "        # -----------------------------------\n",
    "        X = self.vect.fit_transform(df[\"X\"]).toarray()\n",
    "        # transform to dataframe\n",
    "        # X = pd.DataFrame(\n",
    "        #     X, columns=self.vect.get_feature_names_out())\n",
    "        y = df['y'].astype('category')\n",
    "\n",
    "        print(f\"\\n{X.shape}\\n{y.shape}\")\n",
    "\n",
    "        # split dataset, random_state should only set in test\n",
    "        X_v, X_test, y_v, y_test = \\\n",
    "            train_test_split(X, y, train_size=0.8, stratify=y)\n",
    "\n",
    "        X_train, X_vaild, y_train, y_vaild = \\\n",
    "            train_test_split(X_v, y_v,\n",
    "                             train_size=0.75, stratify=y_v)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test, X_vaild, y_vaild\n",
    "\n",
    "    def training(self):\n",
    "        X_train, X_test, y_train, y_test, X_vaild, y_vaild = \\\n",
    "            self.__loadCorpusAndSplit(**self.segParams)\n",
    "\n",
    "        print(f'train dataset shape:{X_train.shape} {y_train.shape}')\n",
    "        print(f'vaild dataset shape:{X_vaild.shape} {y_vaild.shape}')\n",
    "        print(f'test  dataset shape:{X_test.shape} {y_test.shape}')\n",
    "\n",
    "        model = None\n",
    "        if self.modelSelect == \"NB\":\n",
    "            # model = GaussianNB()\n",
    "            model = MultinomialNB(**self.modelParams)\n",
    "        elif self.modelSelect == \"RF\":\n",
    "            model = RandomForestClassifier(**self.modelParams)\n",
    "        elif self.modelSelect == \"XG\":\n",
    "            model = XGBClassifier(**self.modelParams)\n",
    "        \n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # cv = cross_val_score(model, X_train, y_train,\n",
    "        #                      cv=5, scoring='accuracy').mean()\n",
    "        # ----------------------------------------------------------------\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        confusion_matrix_train = metrics.confusion_matrix(\n",
    "            y_train, y_train_pred)\n",
    "        accuracy_score_train = metrics.accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "        y_vaild_pred = model.predict(X_vaild)\n",
    "        confusion_matrix_vaild = metrics.confusion_matrix(\n",
    "            y_vaild, y_vaild_pred)\n",
    "        accuracy_score_vaild = metrics.accuracy_score(y_vaild, y_vaild_pred)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy_score_test = metrics.accuracy_score(y_test, y_pred)\n",
    "        confusion_matrix_test = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        return (model, accuracy_score_train, confusion_matrix_train,\n",
    "                accuracy_score_vaild, confusion_matrix_vaild, accuracy_score_test, confusion_matrix_test)\n",
    "\n",
    "\n",
    "def ml_call(vectParams, segParams, modelSelect, modelParams, resultTimestamp, dumpModel):\n",
    "    nmt = nlp_model_training(vectParams, segParams, modelSelect, modelParams)\n",
    "    model, accuracy_score_train, confusion_matrix_train, accuracy_score_vaild, confusion_matrix_vaild,\\\n",
    "        accuracy_score_test, confusion_matrix_test = nmt.training()\n",
    "\n",
    "    if dumpModel == True:\n",
    "        dump(\n",
    "            model, f'nlpModel_{modelSelect}/{resultTimestamp}.joblib')\n",
    "        dump(\n",
    "            nmt.vect, f'nlpModel_{modelSelect}/vect_{resultTimestamp}.vect')\n",
    "\n",
    "    temp = f\"\\n{resultTimestamp}\\n{vectParams}\\n{segParams}\\n{modelSelect}\\n{modelParams}\\n\" +\\\n",
    "        \"-------------------------------------------------------------\\n\"\n",
    "    with open(f\"./info/{modelSelect}_parameters.txt\", mode=\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(temp)\n",
    "    print(temp)\n",
    "\n",
    "    temp = f\"\\n{resultTimestamp}\\n\" +\\\n",
    "        f\"accuracy score train:{accuracy_score_train:.3f}\\nconfusion matrix train\\n{confusion_matrix_train}\\n\" +\\\n",
    "        f\"accuracy score vaild:{accuracy_score_vaild:.3f}\\nconfusion matrix vaild\\n{confusion_matrix_vaild}\\n\" +\\\n",
    "        f\"accuracy score test:{accuracy_score_test:.3f}\\nconfusion matrix test\\n{confusion_matrix_test}\\n\" +\\\n",
    "        \"-------------------------------------------------------------\\n\"\n",
    "\n",
    "    with open(f\"./info/{modelSelect}_modelScore.txt\", 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(temp)\n",
    "    print(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbdcd35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2156, 1134)\n",
      "(2156,)\n",
      "train dataset shape:(1293, 1134) (1293,)\n",
      "vaild dataset shape:(431, 1134) (431,)\n",
      "test  dataset shape:(432, 1134) (432,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keras/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:09:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "1644656977.435214\n",
      "{'analyzer': 'char_wb', 'max_df': 0.8, 'min_df': 0.0, 'binary': False}\n",
      "{'corpus': './corpus_words/corpus_new.xlsx', 'HMM': True, 'use_paddle': False}\n",
      "XG\n",
      "{'n_estimators': 100, 'max_depth': 10, 'booster': 'gbtree', 'learning_rate': 0.3, 'gpu_id': 0}\n",
      "-------------------------------------------------------------\n",
      "\n",
      "\n",
      "1644656977.435214\n",
      "accuracy score train:0.961\n",
      "confusion matrix train\n",
      "[[104   0   2   1   1]\n",
      " [  1  77   0   1   0]\n",
      " [  0   0 130   6   3]\n",
      " [  0   1   1 335  11]\n",
      " [  0   0   0  23 596]]\n",
      "accuracy score vaild:0.626\n",
      "confusion matrix vaild\n",
      "[[ 15   6   6   2   7]\n",
      " [  5   5   8   3   5]\n",
      " [  6   4  12  11  13]\n",
      " [  3   3   4  69  37]\n",
      " [  4   2   6  26 169]]\n",
      "accuracy score test:0.671\n",
      "confusion matrix test\n",
      "[[ 15   4   7   6   4]\n",
      " [  3   8   8   3   4]\n",
      " [  2   4  13  21   7]\n",
      " [  3   1   4  78  30]\n",
      " [  8   0   6  17 176]]\n",
      "-------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    resultTimestamp = f\"{time.time()}\"\n",
    "\n",
    "    vectParams = {\n",
    "        \"analyzer\": \"char_wb\",\n",
    "        \"max_df\": 0.8,\n",
    "        \"min_df\": 0.0,\n",
    "        \"binary\": False\n",
    "    }\n",
    "\n",
    "    segParams = {\n",
    "        \"corpus\": \"./corpus_words/corpus_new.xlsx\",\n",
    "        \"HMM\": True,\n",
    "        \"use_paddle\": False\n",
    "    }\n",
    "\n",
    "    modelSelect = \"XG\"\n",
    "\n",
    "    # alpha:Additive (Laplace/Lidstone) smoothing parameter(0 for no smoothing).\n",
    "    # \"fit_prior\": bool, default = True Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "    modelParams = {\n",
    "        \"n_estimators\":100,\n",
    "        \"max_depth\": 10,\n",
    "        \"booster\": \"gbtree\",#Specify which booster to use: gbtree, gblinear or dart.\n",
    "        \"learning_rate\": 0.3,#學習速率，預設0.3。\n",
    "        \"gpu_id\" :0\n",
    "    }\n",
    "\n",
    "    ml_call(vectParams, segParams,\n",
    "            modelSelect, modelParams, resultTimestamp, dumpModel=False)\n",
    "except Exception as e:\n",
    "    temp = f'\\n{e}\\n{resultTimestamp}\\n{vectParams}\\n{segParams}\\n{modelSelect}\\n{modelParams}'\n",
    "    with open('./info/err.txt', 'a', encoding='utf-8') as f:\n",
    "        f.write(\n",
    "            temp)\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1e20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
