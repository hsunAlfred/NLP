{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a77135-6d4c-4afe-8311-e1417fede228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030/2030(2028,)\n",
      "(2028,)\n",
      "4/4['10點半點 的 餐， 12點 還沒下單，這 速度 也是 醉 了 。' '味道 還 可以' '德國香腸 沒有 太 好吃'\n",
      " '焗土豆 和 檸檬雞 還是 推薦 的 。']\n",
      "[2. 4. 5. 4.]\n",
      "HMM:True, use_paddle:True\n",
      "cv:0.475\n",
      "accuracy score:0.507\n",
      "confusion_matrix\n",
      "[[72 33  0 10  8]\n",
      " [35 57  0 20 18]\n",
      " [ 1  0  0  0  0]\n",
      " [ 9 19  0 57 51]\n",
      " [ 8 10  0 28 71]]\n"
     ]
    }
   ],
   "source": [
    "from math import pi\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import paddle\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "class nlp_model_trainging:\n",
    "    def __init__(self) -> None:\n",
    "        jieba.set_dictionary('dict.txt.big')\n",
    "        paddle.enable_static()\n",
    "        jieba.enable_paddle()\n",
    "\n",
    "        # load stop word\n",
    "        with open(\"stopwords.txt\", encoding='utf-8') as f:\n",
    "            stopwords = f.read()\n",
    "        self.custom_stopwords_list = [i for i in stopwords.split('\\n')]\n",
    "\n",
    "        # self.avoid_word_kind = (\n",
    "        #     'nr', 'PER', 'ns', 'LOC', 's', 'nt', 'ORG', 'nw', 'w', 'TIME')\n",
    "        self.avoid_word_kind = ()\n",
    "\n",
    "    def __loadCorpusAndTransform(self, corpus, HMM=True, use_paddle=True):\n",
    "        # load corpus(data)\n",
    "        df = pd.read_csv(corpus, on_bad_lines='skip', encoding='utf-8')\n",
    "\n",
    "        # Feature Engineering, let label be binary\n",
    "        # set feature and label\n",
    "        # X = df[\"comment\"].apply(lambda x:  \" \".join(\n",
    "        #     jieba.posseg.cut(str(x), use_paddle=True)))\n",
    "\n",
    "        X = self.__featureTransform(\n",
    "            df[\"comment\"], HMM=HMM, use_paddle=use_paddle)\n",
    "\n",
    "        # y = df[\"star\"].apply(lambda x: 1 if x > 3 else 0)\n",
    "        y = df[\"star\"]\n",
    "\n",
    "        df = pd.DataFrame([X, y], index=[\"X\", 'y']).T.dropna()\n",
    "\n",
    "        X = df[\"X\"]\n",
    "        y = df['y'].astype('category')\n",
    "\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "\n",
    "        # split dataset, random_state should only set in test\n",
    "        return train_test_split(X, y, random_state=1)\n",
    "\n",
    "    def __featureTransform(self, waitTransform, HMM=True, use_paddle=True):\n",
    "        trans = []\n",
    "        n = 1\n",
    "        for i in waitTransform:\n",
    "            print(f'\\r{n}/{len(waitTransform)}', end='')\n",
    "            n += 1\n",
    "            pc = pseg.lcut(str(i), HMM=HMM, use_paddle=use_paddle)\n",
    "            temp = []\n",
    "            for j in pc:\n",
    "                if tuple(j)[1] not in self.avoid_word_kind:\n",
    "                    temp.append(tuple(j)[0])\n",
    "            trans.append(' '.join(temp))\n",
    "        return np.array(trans)\n",
    "\n",
    "    def nlp_NB(self, corpus, HMM=True, use_paddle=True):\n",
    "        X_train, X_test, y_train, y_test = self.__loadCorpusAndTransform(\n",
    "            corpus, HMM=HMM, use_paddle=use_paddle)\n",
    "\n",
    "        # BoW transform\n",
    "        max_df = 0.8  # too high prob to appear\n",
    "        min_df = 3  # too low prob to appear\n",
    "\n",
    "        vect = CountVectorizer(max_df=max_df,\n",
    "                               min_df=min_df,\n",
    "                               token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',\n",
    "                               stop_words=frozenset(self.custom_stopwords_list))\n",
    "\n",
    "        # when develop, it helps us to makesure stop word work\n",
    "        # counts = pd.DataFrame(vect.fit_transform(X_train).toarray(),\n",
    "        #                       columns=vect.get_feature_names_out())\n",
    "\n",
    "        nb = MultinomialNB()\n",
    "        pipe = make_pipeline(vect, nb)\n",
    "\n",
    "        pipe.fit(X_train, y_train)\n",
    "\n",
    "        cv = cross_val_score(pipe, X_train, y_train,\n",
    "                             cv=10, scoring='accuracy').mean()\n",
    "\n",
    "        y_pred = pipe.predict(X_test)\n",
    "\n",
    "        tempX = ('10點半點的餐，12點還沒下單，這速度也是醉了。',\n",
    "                 '味道還可以', '德國香腸沒有太好吃', '焗土豆和檸檬雞還是推薦的。')\n",
    "        tex = self.__featureTransform(tempX)\n",
    "        print(tex)\n",
    "        te = pipe.predict(tex)\n",
    "        print(te)\n",
    "\n",
    "        accuracy_score = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "        confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        return pipe, cv, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pipe, cv, accuracy_score, confusion_matrix = nlp_model_trainging().nlp_NB(\n",
    "        corpus='comment_zh_tw.csv', HMM=True, use_paddle=True)\n",
    "    print(\n",
    "        f'HMM:True, use_paddle:True\\ncv:{cv:5.3f}\\naccuracy score:{accuracy_score:5.3f}\\nconfusion_matrix\\n{confusion_matrix}')\n",
    "    dump(pipe, './selfModel/nlp_NB_HMM_True_paddle_True.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d1dc030-b1c7-4bb3-9c06-a03ac1d0ac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_NB = load('./selfModel/nlp_NB_HMM_True_paddle_True.joblib')\n",
    "def featureTransform(waitTransform, HMM=True, use_paddle=True):\n",
    "        trans = []\n",
    "        n = 1\n",
    "        for i in waitTransform:\n",
    "            print(f'\\r{n}/{len(waitTransform)}', end='')\n",
    "            n += 1\n",
    "            pc = pseg.lcut(str(i), HMM=HMM, use_paddle=use_paddle)\n",
    "            temp = []\n",
    "            for j in pc:\n",
    "                temp.append(tuple(j)[0])\n",
    "            trans.append(' '.join(temp))\n",
    "        return np.array(trans)\n",
    "nlp_NB.predict(featureTransform(['苦瓜難吃','番茄真的太棒了']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073fdb7-2986-400f-b465-451814d32ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
